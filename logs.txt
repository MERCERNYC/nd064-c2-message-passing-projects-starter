* 
* ==> Audit <==
* |---------|------|---------|------|---------|------------|----------|
| Command | Args | Profile | User | Version | Start Time | End Time |
|---------|------|---------|------|---------|------------|----------|
|---------|------|---------|------|---------|------------|----------|

* 
* ==> Last Start <==
* Log file created at: 2021/10/22 23:40:03
Running on machine: Julianas-MBP
Binary: Built with gc go1.17.1 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1022 23:40:03.865803    6591 out.go:298] Setting OutFile to fd 1 ...
I1022 23:40:03.866893    6591 out.go:350] isatty.IsTerminal(1) = true
I1022 23:40:03.866900    6591 out.go:311] Setting ErrFile to fd 2...
I1022 23:40:03.866907    6591 out.go:350] isatty.IsTerminal(2) = true
I1022 23:40:03.868081    6591 root.go:313] Updating PATH: /Users/julianafogg/.minikube/bin
W1022 23:40:03.868703    6591 root.go:291] Error reading config file at /Users/julianafogg/.minikube/config/config.json: open /Users/julianafogg/.minikube/config/config.json: no such file or directory
I1022 23:40:03.870770    6591 out.go:305] Setting JSON to false
I1022 23:40:04.057052    6591 start.go:111] hostinfo: {"hostname":"Julianas-MBP.lan","uptime":113334,"bootTime":1634847069,"procs":290,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"10.14.6","kernelVersion":"18.7.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c41337a1-0ec3-3896-a954-a1f85e849d53"}
W1022 23:40:04.057551    6591 start.go:119] gopshost.Virtualization returned error: not implemented yet
I1022 23:40:04.244038    6591 out.go:177] 😄  minikube v1.23.2 on Darwin 10.14.6
I1022 23:40:04.291533    6591 notify.go:169] Checking for updates...
I1022 23:40:04.436536    6591 out.go:177]     ▪ KUBECONFIG=/etc/kubernetes/admin.conf
I1022 23:40:04.511047    6591 driver.go:343] Setting default libvirt URI to qemu:///system
I1022 23:40:04.511101    6591 global.go:111] Querying for installed drivers using PATH=/Users/julianafogg/.minikube/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin
I1022 23:40:04.511414    6591 global.go:119] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/}
I1022 23:40:04.511529    6591 global.go:119] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/}
I1022 23:40:04.511834    6591 global.go:119] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/}
I1022 23:40:04.511855    6591 global.go:119] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I1022 23:40:05.317302    6591 global.go:119] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I1022 23:40:05.317508    6591 global.go:119] vmware default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in $PATH Reason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/}
I1022 23:40:05.317528    6591 global.go:119] vmwarefusion default: false priority: 1, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:the 'vmwarefusion' driver is no longer available Reason: Fix:Switch to the newer 'vmware' driver by using '--driver=vmware'. This may require first deleting your existing cluster Doc:https://minikube.sigs.k8s.io/docs/drivers/vmware/}
I1022 23:40:10.214915    6591 docker.go:132] docker version: linux-20.10.8
I1022 23:40:10.218815    6591 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1022 23:40:16.555680    6591 cli_runner.go:168] Completed: docker system info --format "{{json .}}": (6.336573293s)
I1022 23:40:16.583463    6591 info.go:263] docker info: {ID:UYYR:CKNE:OARC:NX34:D2AS:IIY7:4AMD:MULG:T2AP:XPHW:Z22X:6XHN Containers:24 ContainersRunning:19 ContainersPaused:0 ContainersStopped:5 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:206 OomKillDisable:true NGoroutines:184 SystemTime:2021-10-23 03:40:10.7309458 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:3 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:2082639872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.8 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e25210fe30a0a703442421b0f60afac609f950a3 Expected:e25210fe30a0a703442421b0f60afac609f950a3} RuncCommit:{ID:v1.0.1-0-g4144b63 Expected:v1.0.1-0-g4144b63} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.0.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.8.0]] Warnings:<nil>}}
I1022 23:40:16.584664    6591 global.go:119] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I1022 23:40:16.584719    6591 driver.go:278] not recommending "ssh" due to default: false
I1022 23:40:16.584764    6591 driver.go:313] Picked: docker
I1022 23:40:16.586120    6591 driver.go:314] Alternatives: [virtualbox ssh]
I1022 23:40:16.586218    6591 driver.go:315] Rejects: [hyperkit podman vmware vmwarefusion parallels]
I1022 23:40:16.867217    6591 out.go:177] ✨  Automatically selected the docker driver. Other choices: virtualbox, ssh
I1022 23:40:16.873647    6591 start.go:278] selected driver: docker
I1022 23:40:16.873665    6591 start.go:751] validating driver "docker" against <nil>
I1022 23:40:16.873802    6591 start.go:762] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I1022 23:40:16.883808    6591 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1022 23:40:18.512319    6591 cli_runner.go:168] Completed: docker system info --format "{{json .}}": (1.628434146s)
I1022 23:40:18.512485    6591 info.go:263] docker info: {ID:UYYR:CKNE:OARC:NX34:D2AS:IIY7:4AMD:MULG:T2AP:XPHW:Z22X:6XHN Containers:24 ContainersRunning:18 ContainersPaused:0 ContainersStopped:6 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:207 OomKillDisable:true NGoroutines:191 SystemTime:2021-10-23 03:40:18.1480434 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:3 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:2082639872 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.8 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e25210fe30a0a703442421b0f60afac609f950a3 Expected:e25210fe30a0a703442421b0f60afac609f950a3} RuncCommit:{ID:v1.0.1-0-g4144b63 Expected:v1.0.1-0-g4144b63} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.0.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.8.0]] Warnings:<nil>}}
I1022 23:40:18.512808    6591 start_flags.go:264] no existing cluster config was found, will generate one from the flags 
I1022 23:40:18.587315    6591 start_flags.go:345] Using suggested 1986MB memory alloc based on sys=8192MB, container=1986MB
I1022 23:40:18.588749    6591 start_flags.go:719] Wait components to verify : map[apiserver:true system_pods:true]
I1022 23:40:18.589303    6591 cni.go:93] Creating CNI manager for ""
I1022 23:40:18.589742    6591 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1022 23:40:18.589791    6591 start_flags.go:278] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de Memory:1986 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.99.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0}
I1022 23:40:18.720172    6591 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1022 23:40:18.738003    6591 cache.go:118] Beginning downloading kic base image for docker with docker
I1022 23:40:18.742098    6591 out.go:177] 🚜  Pulling base image ...
I1022 23:40:18.746004    6591 preload.go:131] Checking if preload exists for k8s version v1.22.2 and runtime docker
I1022 23:40:18.746020    6591 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de in local docker daemon
I1022 23:40:18.845203    6591 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4
I1022 23:40:18.845564    6591 cache.go:57] Caching tarball of preloaded images
I1022 23:40:18.848723    6591 preload.go:131] Checking if preload exists for k8s version v1.22.2 and runtime docker
I1022 23:40:18.859637    6591 out.go:177] 💾  Downloading Kubernetes v1.22.2 preload ...
I1022 23:40:18.859687    6591 preload.go:237] getting checksum for preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4 ...
I1022 23:40:19.058334    6591 download.go:92] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4?checksum=md5:791e709bf7f78c6d1b19eaac4c47421b -> /Users/julianafogg/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4
I1022 23:40:19.251222    6591 cache.go:146] Downloading gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de to local cache
I1022 23:40:19.251798    6591 image.go:59] Checking for gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de in local cache directory
I1022 23:40:19.252006    6591 image.go:119] Writing gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de to local cache
I1022 23:40:53.223553    6591 preload.go:247] saving checksum for preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4 ...
I1022 23:40:53.226138    6591 preload.go:254] verifying checksumm of /Users/julianafogg/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4 ...
I1022 23:40:59.240670    6591 cache.go:149] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de as a tarball
I1022 23:40:59.240705    6591 cache.go:160] Loading gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de from local cache
I1022 23:40:59.243576    6591 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.2 on docker
I1022 23:40:59.448019    6591 profile.go:148] Saving config to /Users/julianafogg/.minikube/profiles/minikube/config.json ...
I1022 23:40:59.462324    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/config.json: {Name:mk2c41a33475f6529bd339d5fe67e05cfe3847b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:49:51.125765    6591 cache.go:163] successfully loaded gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de from cached tarball
I1022 23:49:51.132367    6591 cache.go:206] Successfully downloaded all kic artifacts
I1022 23:49:51.143637    6591 start.go:313] acquiring machines lock for minikube: {Name:mk4aed1d790eb973eab2e8b70c465f892ad741b0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1022 23:49:51.156551    6591 start.go:317] acquired machines lock for "minikube" in 6.132545ms
I1022 23:49:51.158450    6591 start.go:89] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de Memory:1986 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.99.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.22.2 ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0} &{Name: IP: Port:8443 KubernetesVersion:v1.22.2 ControlPlane:true Worker:true}
I1022 23:49:51.170801    6591 start.go:126] createHost starting for "" (driver="docker")
I1022 23:49:51.297941    6591 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=1986MB) ...
I1022 23:49:51.316515    6591 start.go:160] libmachine.API.Create for "minikube" (driver="docker")
I1022 23:49:51.320262    6591 client.go:168] LocalClient.Create starting
I1022 23:49:51.372685    6591 main.go:130] libmachine: Creating CA: /Users/julianafogg/.minikube/certs/ca.pem
I1022 23:49:51.855297    6591 main.go:130] libmachine: Creating client certificate: /Users/julianafogg/.minikube/certs/cert.pem
I1022 23:49:53.089236    6591 cli_runner.go:115] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1022 23:49:57.296247    6591 cli_runner.go:162] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1022 23:49:57.298138    6591 cli_runner.go:168] Completed: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}": (4.205045655s)
I1022 23:49:57.300587    6591 network_create.go:255] running [docker network inspect minikube] to gather additional debugging logs...
I1022 23:49:57.301097    6591 cli_runner.go:115] Run: docker network inspect minikube
W1022 23:49:58.173897    6591 cli_runner.go:162] docker network inspect minikube returned with exit code 1
I1022 23:49:58.173952    6591 network_create.go:258] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1022 23:49:58.178894    6591 network_create.go:260] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1022 23:49:58.178991    6591 cli_runner.go:115] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1022 23:49:59.207590    6591 network.go:288] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc000528380] misses:0}
I1022 23:49:59.208506    6591 network.go:235] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1022 23:49:59.227780    6591 network_create.go:106] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1022 23:49:59.236607    6591 cli_runner.go:115] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true minikube
I1022 23:50:06.447740    6591 cli_runner.go:168] Completed: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true minikube: (7.208271072s)
I1022 23:50:06.448419    6591 network_create.go:90] docker network minikube 192.168.49.0/24 created
I1022 23:50:06.451259    6591 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I1022 23:50:06.452014    6591 cli_runner.go:115] Run: docker ps -a --format {{.Names}}
I1022 23:50:07.718122    6591 cli_runner.go:168] Completed: docker ps -a --format {{.Names}}: (1.266028513s)
I1022 23:50:07.720496    6591 cli_runner.go:115] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1022 23:50:09.164549    6591 cli_runner.go:168] Completed: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true: (1.443554014s)
I1022 23:50:09.170105    6591 oci.go:102] Successfully created a docker volume minikube
I1022 23:50:09.170255    6591 cli_runner.go:115] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de -d /var/lib
I1022 23:50:44.041841    6591 cli_runner.go:168] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de -d /var/lib: (34.868741181s)
I1022 23:50:44.043533    6591 oci.go:106] Successfully prepared a docker volume minikube
I1022 23:50:44.050640    6591 cli_runner.go:115] Run: docker info --format "'{{json .SecurityOptions}}'"
I1022 23:50:44.052425    6591 preload.go:131] Checking if preload exists for k8s version v1.22.2 and runtime docker
I1022 23:50:44.075228    6591 kic.go:179] Starting extracting preloaded images to volume ...
I1022 23:50:44.086052    6591 cli_runner.go:115] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/julianafogg/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de -I lz4 -xf /preloaded.tar -C /extractDir
I1022 23:50:47.952552    6591 cli_runner.go:168] Completed: docker info --format "'{{json .SecurityOptions}}'": (3.901764452s)
I1022 23:50:47.987131    6591 cli_runner.go:115] Run: docker run -d -t --privileged --device /dev/fuse --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=1986mb --memory-swap=1986mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de
I1022 23:50:58.269357    6591 cli_runner.go:168] Completed: docker run -d -t --privileged --device /dev/fuse --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=1986mb --memory-swap=1986mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de: (10.281057061s)
I1022 23:50:58.271667    6591 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Running}}
I1022 23:50:59.807519    6591 cli_runner.go:168] Completed: docker container inspect minikube --format={{.State.Running}}: (1.535714066s)
I1022 23:50:59.807766    6591 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:51:00.310163    6591 cli_runner.go:115] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1022 23:51:03.532889    6591 cli_runner.go:168] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (3.222451001s)
I1022 23:51:03.533638    6591 oci.go:281] the created container "minikube" has a running status.
I1022 23:51:03.534294    6591 kic.go:210] Creating ssh key for kic: /Users/julianafogg/.minikube/machines/minikube/id_rsa...
I1022 23:51:03.888105    6591 kic_runner.go:188] docker (temp): /Users/julianafogg/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1022 23:51:12.603181    6591 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:51:13.425733    6591 kic_runner.go:94] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1022 23:51:13.425751    6591 kic_runner.go:115] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1022 23:51:15.487262    6591 kic_runner.go:124] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (2.060910915s)
I1022 23:52:05.553161    6591 cli_runner.go:168] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/julianafogg/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.2-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de -I lz4 -xf /preloaded.tar -C /extractDir: (1m21.461126038s)
I1022 23:52:05.553883    6591 kic.go:188] duration metric: took 81.477793 seconds to extract preloaded images to volume
I1022 23:52:05.555183    6591 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1022 23:52:06.521933    6591 machine.go:88] provisioning docker machine ...
I1022 23:52:06.527417    6591 ubuntu.go:169] provisioning hostname "minikube"
I1022 23:52:06.533809    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:06.951480    6591 main.go:130] libmachine: Using SSH client type: native
I1022 23:52:06.956538    6591 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x13985c0] 0x139b6a0 <nil>  [] 0s} 127.0.0.1 58099 <nil> <nil>}
I1022 23:52:06.956557    6591 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1022 23:52:08.203292    6591 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I1022 23:52:08.203928    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:08.584034    6591 main.go:130] libmachine: Using SSH client type: native
I1022 23:52:08.585120    6591 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x13985c0] 0x139b6a0 <nil>  [] 0s} 127.0.0.1 58099 <nil> <nil>}
I1022 23:52:08.585169    6591 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1022 23:52:08.833229    6591 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1022 23:52:08.833286    6591 ubuntu.go:175] set auth options {CertDir:/Users/julianafogg/.minikube CaCertPath:/Users/julianafogg/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/julianafogg/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/julianafogg/.minikube/machines/server.pem ServerKeyPath:/Users/julianafogg/.minikube/machines/server-key.pem ClientKeyPath:/Users/julianafogg/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/julianafogg/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/julianafogg/.minikube}
I1022 23:52:08.833351    6591 ubuntu.go:177] setting up certificates
I1022 23:52:08.833717    6591 provision.go:83] configureAuth start
I1022 23:52:08.834920    6591 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:52:09.277668    6591 provision.go:138] copyHostCerts
I1022 23:52:09.278118    6591 exec_runner.go:152] cp: /Users/julianafogg/.minikube/certs/ca.pem --> /Users/julianafogg/.minikube/ca.pem (1090 bytes)
I1022 23:52:09.278913    6591 exec_runner.go:152] cp: /Users/julianafogg/.minikube/certs/cert.pem --> /Users/julianafogg/.minikube/cert.pem (1135 bytes)
I1022 23:52:09.280438    6591 exec_runner.go:152] cp: /Users/julianafogg/.minikube/certs/key.pem --> /Users/julianafogg/.minikube/key.pem (1675 bytes)
I1022 23:52:09.281821    6591 provision.go:112] generating server cert: /Users/julianafogg/.minikube/machines/server.pem ca-key=/Users/julianafogg/.minikube/certs/ca.pem private-key=/Users/julianafogg/.minikube/certs/ca-key.pem org=julianafogg.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1022 23:52:09.895878    6591 provision.go:172] copyRemoteCerts
I1022 23:52:09.897497    6591 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1022 23:52:09.898136    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:12.195557    6591 cli_runner.go:168] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (2.297275422s)
I1022 23:52:12.196289    6591 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58099 SSHKeyPath:/Users/julianafogg/.minikube/machines/minikube/id_rsa Username:docker}
I1022 23:52:12.989405    6591 ssh_runner.go:192] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (3.09151339s)
I1022 23:52:12.992450    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I1022 23:52:13.892878    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1022 23:52:14.617376    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1022 23:52:14.836081    6591 provision.go:86] duration metric: configureAuth took 5.999536975s
I1022 23:52:14.836110    6591 ubuntu.go:193] setting minikube options for container-runtime
I1022 23:52:14.851759    6591 config.go:177] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.2
I1022 23:52:14.852060    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:15.518168    6591 main.go:130] libmachine: Using SSH client type: native
I1022 23:52:15.518776    6591 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x13985c0] 0x139b6a0 <nil>  [] 0s} 127.0.0.1 58099 <nil> <nil>}
I1022 23:52:15.518869    6591 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1022 23:52:15.996933    6591 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I1022 23:52:15.997257    6591 ubuntu.go:71] root file system type: overlay
I1022 23:52:16.000051    6591 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1022 23:52:16.000665    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:16.509469    6591 main.go:130] libmachine: Using SSH client type: native
I1022 23:52:16.509990    6591 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x13985c0] 0x139b6a0 <nil>  [] 0s} 127.0.0.1 58099 <nil> <nil>}
I1022 23:52:16.510857    6591 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1022 23:52:17.105226    6591 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1022 23:52:17.105656    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:17.953783    6591 main.go:130] libmachine: Using SSH client type: native
I1022 23:52:17.954101    6591 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x13985c0] 0x139b6a0 <nil>  [] 0s} 127.0.0.1 58099 <nil> <nil>}
I1022 23:52:17.954140    6591 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1022 23:52:38.452799    6591 main.go:130] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2021-07-30 19:52:33.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2021-10-23 03:52:17.173509000 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
+BindsTo=containerd.service
 After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1022 23:52:38.452989    6591 machine.go:91] provisioned docker machine in 31.930120508s
I1022 23:52:38.453963    6591 client.go:171] LocalClient.Create took 2m47.1283578s
I1022 23:52:38.455372    6591 start.go:168] duration metric: libmachine.API.Create for "minikube" took 2m47.133517567s
I1022 23:52:38.455814    6591 start.go:267] post-start starting for "minikube" (driver="docker")
I1022 23:52:38.456134    6591 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1022 23:52:38.458144    6591 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1022 23:52:38.458983    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:40.112497    6591 cli_runner.go:168] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.652611148s)
I1022 23:52:40.112546    6591 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58099 SSHKeyPath:/Users/julianafogg/.minikube/machines/minikube/id_rsa Username:docker}
I1022 23:52:40.528452    6591 ssh_runner.go:192] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (2.070152831s)
I1022 23:52:40.529361    6591 ssh_runner.go:152] Run: cat /etc/os-release
I1022 23:52:40.557189    6591 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1022 23:52:40.557220    6591 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1022 23:52:40.557238    6591 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1022 23:52:40.557250    6591 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I1022 23:52:40.562953    6591 filesync.go:126] Scanning /Users/julianafogg/.minikube/addons for local assets ...
I1022 23:52:40.575189    6591 filesync.go:126] Scanning /Users/julianafogg/.minikube/files for local assets ...
I1022 23:52:40.576632    6591 start.go:270] post-start completed in 2.120436348s
I1022 23:52:40.578454    6591 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:52:40.972224    6591 profile.go:148] Saving config to /Users/julianafogg/.minikube/profiles/minikube/config.json ...
I1022 23:52:40.973574    6591 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1022 23:52:40.973677    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:41.355381    6591 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58099 SSHKeyPath:/Users/julianafogg/.minikube/machines/minikube/id_rsa Username:docker}
I1022 23:52:41.639071    6591 start.go:129] duration metric: createHost completed in 2m50.461664592s
I1022 23:52:41.639319    6591 start.go:80] releasing machines lock for "minikube", held for 2m50.477982772s
I1022 23:52:41.640266    6591 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1022 23:52:42.043257    6591 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I1022 23:52:42.044167    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:42.044274    6591 ssh_runner.go:152] Run: systemctl --version
I1022 23:52:42.044344    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1022 23:52:42.438639    6591 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58099 SSHKeyPath:/Users/julianafogg/.minikube/machines/minikube/id_rsa Username:docker}
I1022 23:52:42.465757    6591 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58099 SSHKeyPath:/Users/julianafogg/.minikube/machines/minikube/id_rsa Username:docker}
I1022 23:52:42.917278    6591 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I1022 23:52:43.541463    6591 ssh_runner.go:192] Completed: curl -sS -m 2 https://k8s.gcr.io/: (1.49813619s)
I1022 23:52:43.541606    6591 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1022 23:52:43.702769    6591 cruntime.go:255] skipping containerd shutdown because we are bound to it
I1022 23:52:43.702941    6591 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I1022 23:52:43.738626    6591 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1022 23:52:43.813738    6591 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I1022 23:52:44.182639    6591 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I1022 23:52:44.753883    6591 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1022 23:52:44.803932    6591 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I1022 23:52:45.177836    6591 ssh_runner.go:152] Run: sudo systemctl start docker
I1022 23:52:45.245979    6591 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1022 23:52:47.683472    6591 ssh_runner.go:192] Completed: docker version --format {{.Server.Version}}: (2.437395989s)
I1022 23:52:47.683546    6591 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1022 23:52:48.033402    6591 out.go:204] 🐳  Preparing Kubernetes v1.22.2 on Docker 20.10.8 ...
I1022 23:52:48.034288    6591 cli_runner.go:115] Run: docker exec -t minikube dig +short host.docker.internal
I1022 23:52:51.169331    6591 cli_runner.go:168] Completed: docker exec -t minikube dig +short host.docker.internal: (3.134854735s)
I1022 23:52:51.169359    6591 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1022 23:52:51.170378    6591 ssh_runner.go:152] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1022 23:52:51.215433    6591 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:52:51.356261    6591 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1022 23:52:51.716391    6591 preload.go:131] Checking if preload exists for k8s version v1.22.2 and runtime docker
I1022 23:52:51.716528    6591 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1022 23:52:52.247523    6591 docker.go:558] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.22.2
k8s.gcr.io/kube-controller-manager:v1.22.2
k8s.gcr.io/kube-scheduler:v1.22.2
k8s.gcr.io/kube-proxy:v1.22.2
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5

-- /stdout --
I1022 23:52:52.247542    6591 docker.go:489] Images already preloaded, skipping extraction
I1022 23:52:52.247925    6591 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1022 23:52:52.551877    6591 docker.go:558] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.22.2
k8s.gcr.io/kube-controller-manager:v1.22.2
k8s.gcr.io/kube-scheduler:v1.22.2
k8s.gcr.io/kube-proxy:v1.22.2
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5

-- /stdout --
I1022 23:52:52.551913    6591 cache_images.go:78] Images are preloaded, skipping loading
I1022 23:52:52.552541    6591 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I1022 23:52:55.070742    6591 ssh_runner.go:192] Completed: docker info --format {{.CgroupDriver}}: (2.518102601s)
I1022 23:52:55.076387    6591 cni.go:93] Creating CNI manager for ""
I1022 23:52:55.076640    6591 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1022 23:52:55.077654    6591 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1022 23:52:55.077856    6591 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.2 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1022 23:52:55.080653    6591 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.2
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1022 23:52:55.107168    6591 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.2/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1022 23:52:55.107276    6591 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.2
I1022 23:52:55.193350    6591 binaries.go:44] Found k8s binaries, skipping transfer
I1022 23:52:55.194272    6591 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1022 23:52:55.222630    6591 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I1022 23:52:55.299797    6591 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1022 23:52:55.386379    6591 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2051 bytes)
I1022 23:52:55.470373    6591 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1022 23:52:55.487227    6591 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1022 23:52:55.520853    6591 certs.go:52] Setting up /Users/julianafogg/.minikube/profiles/minikube for IP: 192.168.49.2
I1022 23:52:55.521361    6591 certs.go:183] generating minikubeCA CA: /Users/julianafogg/.minikube/ca.key
I1022 23:52:56.036804    6591 crypto.go:157] Writing cert to /Users/julianafogg/.minikube/ca.crt ...
I1022 23:52:56.036847    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/ca.crt: {Name:mk64723a80358d0484eb1f2939ba3a67a8fdce77 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.037649    6591 crypto.go:165] Writing key to /Users/julianafogg/.minikube/ca.key ...
I1022 23:52:56.038629    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/ca.key: {Name:mk65fad2b7e3a2cbb0c73fc9c68d78a58c7028b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.039381    6591 certs.go:183] generating proxyClientCA CA: /Users/julianafogg/.minikube/proxy-client-ca.key
I1022 23:52:56.410950    6591 crypto.go:157] Writing cert to /Users/julianafogg/.minikube/proxy-client-ca.crt ...
I1022 23:52:56.410962    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/proxy-client-ca.crt: {Name:mk90ad2c14be63de546b45b51ec2fcd23556471c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.411351    6591 crypto.go:165] Writing key to /Users/julianafogg/.minikube/proxy-client-ca.key ...
I1022 23:52:56.411358    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/proxy-client-ca.key: {Name:mk3b06b4b8230d75540ab5a46ec6537bc00ad3e0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.412797    6591 certs.go:297] generating minikube-user signed cert: /Users/julianafogg/.minikube/profiles/minikube/client.key
I1022 23:52:56.413073    6591 crypto.go:69] Generating cert /Users/julianafogg/.minikube/profiles/minikube/client.crt with IP's: []
I1022 23:52:56.960054    6591 crypto.go:157] Writing cert to /Users/julianafogg/.minikube/profiles/minikube/client.crt ...
I1022 23:52:56.960076    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/client.crt: {Name:mkfad91ff8bf5a77d88743d8b8e6970776e11bac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.960689    6591 crypto.go:165] Writing key to /Users/julianafogg/.minikube/profiles/minikube/client.key ...
I1022 23:52:56.960699    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/client.key: {Name:mk27c98cdded83aa7c62f9f307d4fa49601f185b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:56.961227    6591 certs.go:297] generating minikube signed cert: /Users/julianafogg/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1022 23:52:56.961233    6591 crypto.go:69] Generating cert /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1022 23:52:57.128097    6591 crypto.go:157] Writing cert to /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1022 23:52:57.128115    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk6bf87a3aed94d4aa5ee94eab9040daa66c8f09 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:57.129038    6591 crypto.go:165] Writing key to /Users/julianafogg/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1022 23:52:57.129053    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk322b9af41d36346b4ab13995fc4289412add26 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:57.129589    6591 certs.go:308] copying /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt
I1022 23:52:57.130269    6591 certs.go:312] copying /Users/julianafogg/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/julianafogg/.minikube/profiles/minikube/apiserver.key
I1022 23:52:57.131428    6591 certs.go:297] generating aggregator signed cert: /Users/julianafogg/.minikube/profiles/minikube/proxy-client.key
I1022 23:52:57.131444    6591 crypto.go:69] Generating cert /Users/julianafogg/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1022 23:52:57.345868    6591 crypto.go:157] Writing cert to /Users/julianafogg/.minikube/profiles/minikube/proxy-client.crt ...
I1022 23:52:57.345884    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/proxy-client.crt: {Name:mk3b3dce0e4aeed21556e53527dd6f34e9fdb66b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:57.347034    6591 crypto.go:165] Writing key to /Users/julianafogg/.minikube/profiles/minikube/proxy-client.key ...
I1022 23:52:57.347068    6591 lock.go:36] WriteFile acquiring /Users/julianafogg/.minikube/profiles/minikube/proxy-client.key: {Name:mk97f4658582539fbfb819ff4a1afa80d73bda3a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1022 23:52:57.350172    6591 certs.go:376] found cert: /Users/julianafogg/.minikube/certs/Users/julianafogg/.minikube/certs/ca-key.pem (1679 bytes)
I1022 23:52:57.350569    6591 certs.go:376] found cert: /Users/julianafogg/.minikube/certs/Users/julianafogg/.minikube/certs/ca.pem (1090 bytes)
I1022 23:52:57.350869    6591 certs.go:376] found cert: /Users/julianafogg/.minikube/certs/Users/julianafogg/.minikube/certs/cert.pem (1135 bytes)
I1022 23:52:57.351209    6591 certs.go:376] found cert: /Users/julianafogg/.minikube/certs/Users/julianafogg/.minikube/certs/key.pem (1675 bytes)
I1022 23:52:57.406393    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1022 23:52:57.523970    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1022 23:52:57.673053    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1022 23:52:57.796839    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1022 23:52:57.892308    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1022 23:52:58.001439    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1022 23:52:58.089549    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1022 23:52:58.258335    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1022 23:52:58.394686    6591 ssh_runner.go:319] scp /Users/julianafogg/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1022 23:52:58.488982    6591 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1022 23:52:58.595879    6591 ssh_runner.go:152] Run: openssl version
I1022 23:52:58.640116    6591 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1022 23:52:58.677735    6591 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1022 23:52:58.699261    6591 certs.go:419] hashing: -rw-r--r-- 1 root root 1111 Oct 23 03:52 /usr/share/ca-certificates/minikubeCA.pem
I1022 23:52:58.699335    6591 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1022 23:52:58.727370    6591 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1022 23:52:58.759682    6591 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.27@sha256:89b4738ee74ba28684676e176752277f0db46f57d27f0e08c3feec89311e22de Memory:1986 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.99.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.2 ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0}
I1022 23:52:58.759856    6591 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1022 23:52:59.253176    6591 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1022 23:52:59.292997    6591 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1022 23:52:59.329083    6591 kubeadm.go:220] ignoring SystemVerification for kubeadm because of docker driver
I1022 23:52:59.329404    6591 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1022 23:52:59.365340    6591 kubeadm.go:151] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1022 23:52:59.367534    6591 ssh_runner.go:243] Start: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1022 23:57:57.171552    6591 out.go:204]     ▪ Generating certificates and keys ...
I1022 23:57:57.282081    6591 out.go:204]     ▪ Booting up control plane ...
W1022 23:57:57.343954    6591 out.go:242] 💢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.22.2
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

	Unfortunately, an error has occurred:
		timed out waiting for the condition

	This error is likely caused by:
		- The kubelet is not running
		- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
		- 'systemctl status kubelet'
		- 'journalctl -xeu kubelet'

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in docker:
		- 'docker ps -a | grep kube | grep -v pause'
		Once you have found the failing container, you can inspect its logs with:
		- 'docker logs CONTAINERID'


stderr:
	[WARNING Swap]: running with swap on is not supported. Please disable swap
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

I1022 23:57:57.363404    6591 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm reset --cri-socket /var/run/dockershim.sock --force"
I1022 23:59:48.931467    6591 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm reset --cri-socket /var/run/dockershim.sock --force": (1m51.564140706s)
I1022 23:59:48.934566    6591 ssh_runner.go:152] Run: sudo systemctl stop -f kubelet
I1022 23:59:49.072456    6591 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1022 23:59:49.263096    6591 kubeadm.go:220] ignoring SystemVerification for kubeadm because of docker driver
I1022 23:59:49.263535    6591 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1022 23:59:49.317535    6591 kubeadm.go:151] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1022 23:59:49.319293    6591 ssh_runner.go:243] Start: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.22.2:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1023 00:03:53.316832    6591 out.go:204]     ▪ Generating certificates and keys ...
I1023 00:03:53.398962    6591 out.go:204]     ▪ Booting up control plane ...
I1023 00:03:53.409350    6591 out.go:204]     ▪ Configuring RBAC rules ...
I1023 00:03:53.427847    6591 cni.go:93] Creating CNI manager for ""
I1023 00:03:53.429689    6591 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1023 00:03:53.436718    6591 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1023 00:03:53.439583    6591 ssh_runner.go:152] Run: sudo /var/lib/minikube/binaries/v1.22.2/kubectl label nodes minikube.k8s.io/version=v1.23.2 minikube.k8s.io/commit=0a0ad764652082477c00d51d2475284b5d39ceed minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2021_10_23T00_03_53_0700 --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1023 00:03:53.440232    6591 ssh_runner.go:152] Run: sudo /var/lib/minikube/binaries/v1.22.2/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1023 00:03:54.227237    6591 ops.go:34] apiserver oom_adj: -16
I1023 00:03:57.737545    6591 ssh_runner.go:192] Completed: sudo /var/lib/minikube/binaries/v1.22.2/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (4.29527706s)
I1023 00:03:57.737902    6591 kubeadm.go:985] duration metric: took 4.303237027s to wait for elevateKubeSystemPrivileges.
I1023 00:03:58.727802    6591 ssh_runner.go:192] Completed: sudo /var/lib/minikube/binaries/v1.22.2/kubectl label nodes minikube.k8s.io/version=v1.23.2 minikube.k8s.io/commit=0a0ad764652082477c00d51d2475284b5d39ceed minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2021_10_23T00_03_53_0700 --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig: (5.286522836s)
I1023 00:03:58.728473    6591 kubeadm.go:392] StartCluster complete in 10m59.950401478s
I1023 00:03:58.731437    6591 settings.go:142] acquiring lock: {Name:mkd828db7b7cf864e2d4d124744c6ee9c2490d8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1023 00:03:58.736521    6591 settings.go:150] Updating kubeconfig:  /etc/kubernetes/admin.conf
I1023 00:03:58.865800    6591 out.go:177] 
W1023 00:03:58.866588    6591 out.go:242] ❌  Exiting due to GUEST_START: Failed kubeconfig update: writing kubeconfig: Error creating directory: /etc/kubernetes: mkdir /etc/kubernetes: permission denied
W1023 00:03:58.866609    6591 out.go:242] 
W1023 00:03:58.889044    6591 out.go:242] [31m╭───────────────────────────────────────────────────────────────────────────────────────────╮[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    😿  If the above advice does not help, please let us know:                             [31m│[0m
[31m│[0m    👉  https://github.com/kubernetes/minikube/issues/new/choose                           [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯[0m

* 
* ==> Docker <==
* -- Logs begin at Sat 2021-10-23 03:51:10 UTC, end at Sat 2021-10-23 04:18:43 UTC. --
Oct 23 03:51:15 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 23 03:51:17 minikube dockerd[217]: time="2021-10-23T03:51:17.991311000Z" level=info msg="Starting up"
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.038285900Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.038379500Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.038442200Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.038513000Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.058870300Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.059504800Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.059877200Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.060245800Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.387460700Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.531431100Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.531949000Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 23 03:51:18 minikube dockerd[217]: time="2021-10-23T03:51:18.544798600Z" level=info msg="Loading containers: start."
Oct 23 03:51:19 minikube dockerd[217]: time="2021-10-23T03:51:19.277761200Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 23 03:51:19 minikube dockerd[217]: time="2021-10-23T03:51:19.610684100Z" level=info msg="Loading containers: done."
Oct 23 03:51:20 minikube dockerd[217]: time="2021-10-23T03:51:20.213473100Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Oct 23 03:51:20 minikube dockerd[217]: time="2021-10-23T03:51:20.224685900Z" level=info msg="Daemon has completed initialization"
Oct 23 03:51:20 minikube systemd[1]: Started Docker Application Container Engine.
Oct 23 03:51:21 minikube dockerd[217]: time="2021-10-23T03:51:21.041573900Z" level=info msg="API listen on /run/docker.sock"
Oct 23 03:52:19 minikube systemd[1]: docker.service: Current command vanished from the unit file, execution of the command list won't be resumed.
Oct 23 03:52:23 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 23 03:52:24 minikube dockerd[217]: time="2021-10-23T03:52:24.074280300Z" level=info msg="Processing signal 'terminated'"
Oct 23 03:52:24 minikube dockerd[217]: time="2021-10-23T03:52:24.450151100Z" level=info msg="Daemon shutdown complete"
Oct 23 03:52:24 minikube systemd[1]: docker.service: Succeeded.
Oct 23 03:52:24 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 23 03:52:24 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.078324100Z" level=info msg="Starting up"
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.400422100Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.401098000Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.401486100Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.414233600Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.668074000Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.668151800Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.668207800Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.668251200Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 23 03:52:33 minikube dockerd[461]: time="2021-10-23T03:52:33.977182700Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 23 03:52:34 minikube dockerd[461]: time="2021-10-23T03:52:34.323727700Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 23 03:52:34 minikube dockerd[461]: time="2021-10-23T03:52:34.324896100Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 23 03:52:34 minikube dockerd[461]: time="2021-10-23T03:52:34.348342700Z" level=info msg="Loading containers: start."
Oct 23 03:52:35 minikube dockerd[461]: time="2021-10-23T03:52:35.586252600Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 23 03:52:35 minikube dockerd[461]: time="2021-10-23T03:52:35.837377900Z" level=info msg="Loading containers: done."
Oct 23 03:52:37 minikube dockerd[461]: time="2021-10-23T03:52:37.763727600Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Oct 23 03:52:37 minikube dockerd[461]: time="2021-10-23T03:52:37.764949600Z" level=info msg="Daemon has completed initialization"
Oct 23 03:52:38 minikube systemd[1]: Started Docker Application Container Engine.
Oct 23 03:52:38 minikube dockerd[461]: time="2021-10-23T03:52:38.530034600Z" level=info msg="API listen on [::]:2376"
Oct 23 03:52:38 minikube dockerd[461]: time="2021-10-23T03:52:38.536201600Z" level=info msg="API listen on /var/run/docker.sock"
Oct 23 03:57:13 minikube dockerd[461]: time="2021-10-23T03:57:08.407861200Z" level=error msg="652f2a4c2440ec984556c0ef409f0fdba00602396215faea18cda3c943c7a3db cleanup: failed to delete container from containerd: cannot delete running task 652f2a4c2440ec984556c0ef409f0fdba00602396215faea18cda3c943c7a3db: failed precondition"
Oct 23 03:59:17 minikube dockerd[461]: time="2021-10-23T03:59:17.491643300Z" level=info msg="ignoring event" container=92622f12da8da72147e8ac39f90245a8c264c4a65a81ea1960ea5d6219578fd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:25 minikube dockerd[461]: time="2021-10-23T03:59:25.408127200Z" level=info msg="ignoring event" container=bef9c5726ba05982eac5e2db187341d26a7fe67a921ace01567bb0d3a0c29471 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:28 minikube dockerd[461]: time="2021-10-23T03:59:28.559344700Z" level=info msg="ignoring event" container=843404310f450863ed13e365ae6910a030b41cf58e741cf334a5867d6aab20ab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:39 minikube dockerd[461]: time="2021-10-23T03:59:39.506672900Z" level=info msg="Container 226b6906c1d8e159821929ac533187f53926659b1d8eca39c5ec3ee3488815de failed to exit within 10 seconds of signal 15 - using the force"
Oct 23 03:59:41 minikube dockerd[461]: time="2021-10-23T03:59:41.342641100Z" level=info msg="ignoring event" container=226b6906c1d8e159821929ac533187f53926659b1d8eca39c5ec3ee3488815de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:43 minikube dockerd[461]: time="2021-10-23T03:59:43.992558600Z" level=info msg="ignoring event" container=6653c45b94d6ec5b17c961c0346d0dfb337748ebff7bee19ad5515b0b7bbe650 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:46 minikube dockerd[461]: time="2021-10-23T03:59:46.300915100Z" level=info msg="ignoring event" container=5950c2e43f8026678e212dbff230a8fc0408ff4d9a9db9524b0002c02ce176a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:47 minikube dockerd[461]: time="2021-10-23T03:59:47.226963200Z" level=info msg="ignoring event" container=84a316883cde176f1fc3ea7c13433b4352f14ff512cdac34aca877e62548e7f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 03:59:48 minikube dockerd[461]: time="2021-10-23T03:59:48.354452000Z" level=info msg="ignoring event" container=770dadc455affffde7e683878498328a502d71c47b7e57a2302554f1c3879017 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 04:01:58 minikube dockerd[461]: time="2021-10-23T04:01:58.247232100Z" level=info msg="ignoring event" container=dc89b9013f9ee3408eed281972089e943921213efa47faa1903ea44f091c8a7b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 23 04:03:43 minikube dockerd[461]: time="2021-10-23T04:03:43.033806500Z" level=info msg="ignoring event" container=37088de6031232a6c024371c006a93fbce32f2258645a5d21e57e6543b4979b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
f0fba4ae8b388       8d147537fb7d1       12 minutes ago      Running             coredns                   0                   f39fdb6fcd774
0eb2cd29a8d73       8d147537fb7d1       12 minutes ago      Running             coredns                   0                   87bf2134e94af
e6048d9bfd749       873127efbc8a7       12 minutes ago      Running             kube-proxy                0                   33b414d2827a2
5910b8d694096       5425bcbd23c54       14 minutes ago      Running             kube-controller-manager   4                   f108ae12f17a1
37088de603123       5425bcbd23c54       16 minutes ago      Exited              kube-controller-manager   3                   f108ae12f17a1
49aa8c6c969f0       b51ddc1014b04       17 minutes ago      Running             kube-scheduler            1                   c3296de51bb1d
b857788c94b90       0048118155842       17 minutes ago      Running             etcd                      1                   9ed4e038b8d86
3c26bb3801010       e64579b7d8862       17 minutes ago      Running             kube-apiserver            1                   851193aa53d87

* 
* ==> coredns [0eb2cd29a8d7] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = db32ca3650231d74073ff4cf814959a7
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.3936865s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.5372559s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.0695203s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.4855295s

* 
* ==> coredns [f0fba4ae8b38] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = db32ca3650231d74073ff4cf814959a7
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.0681345s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.236277s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.1663109s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.092742s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.0939467s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.1378197s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.8985401s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.0006397s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.7330698s

* 
* ==> describe nodes <==
* 
* ==> dmesg <==
* [Oct23 03:06] clocksource: timekeeping watchdog on CPU1: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.339603] clocksource:                       'hpet' wd_now: aec8f7eb wd_last: a89e567c mask: ffffffff
[  +0.067024] clocksource:                       'tsc' cs_now: b2c610989222 cs_last: b2c0e8941882 mask: ffffffffffffffff
[  +1.165146] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.

* 
* ==> etcd [b857788c94b9] <==
* {"level":"warn","ts":"2021-10-23T04:18:37.127Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:38.803Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36858","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:38.892Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36682","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:40.953Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36722","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:41.313Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36664","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36664: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:41.582Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36954","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:41.631Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:43.119Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36896","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:43.322Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36596","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36596: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:43.832Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36958","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:44.213Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36688","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36688: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:44.558Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36662","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36662: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:45.249Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36976","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:45.339Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36980","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36980: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:46.959Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"605.0285ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/servicenodeports\" ","response":"range_response_count:1 size:120"}
{"level":"info","ts":"2021-10-23T04:18:47.216Z","caller":"traceutil/trace.go:171","msg":"trace[1237374754] range","detail":"{range_begin:/registry/ranges/servicenodeports; range_end:; response_count:1; response_revision:827; }","duration":"1.0048432s","start":"2021-10-23T04:18:46.240Z","end":"2021-10-23T04:18:47.070Z","steps":["trace[1237374754] 'range keys from bolt db'  (duration: 566.9243ms)"],"step_count":1}
{"level":"warn","ts":"2021-10-23T04:18:47.337Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-10-23T04:18:46.240Z","time spent":"1.2273594s","remote":"127.0.0.1:46200","response type":"/etcdserverpb.KV/Range","request count":0,"request size":35,"response count":1,"response size":144,"request content":"key:\"/registry/ranges/servicenodeports\" "}
{"level":"warn","ts":"2021-10-23T04:18:47.402Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36990","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:47.562Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37008","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:47.633Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36960","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:36960: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.335Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37292","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.335Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37016","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37016: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.336Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37022","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.336Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37014","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.338Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37260","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.338Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37262","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.339Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37030","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37030: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.339Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37264","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.362Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37024","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.363Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37044","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37044: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.363Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37046","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.363Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37050","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.363Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37052","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37052: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.369Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37120","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37120: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.411Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37238","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.433Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37182","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37182: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.440Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37244","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.464Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37058","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37058: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.533Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37304","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.700Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37230","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.700Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37208","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.707Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37228","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.758Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37222","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.787Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37146","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37146: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.788Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37248","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.822Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37266","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.822Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:36992","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.822Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37176","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.822Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37320","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.857Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37184","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.858Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37112","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37112: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.858Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37010","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37010: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.858Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.859Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37166","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37166: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:49.935Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37118","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.974Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37252","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:49.974Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37254","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:50.108Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37256","server-name":"","error":"EOF"}
{"level":"warn","ts":"2021-10-23T04:18:50.133Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37012","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:37012: read: connection reset by peer"}
{"level":"warn","ts":"2021-10-23T04:18:50.134Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:37258","server-name":"","error":"EOF"}

* 
* ==> kernel <==
*  04:18:56 up  2:00,  0 users,  load average: 22.57, 22.38, 22.27
Linux minikube 5.10.47-linuxkit #1 SMP Sat Jul 3 21:51:47 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [3c26bb380101] <==
* W1023 04:18:11.089176       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:05.950196       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:11.110906       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:11.205395       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
E1023 04:18:18.959363       1 timeout.go:135] post-timeout activity - time-elapsed: 33.2234156s, GET "/api" result: <nil>
E1023 04:18:26.598780       1 timeout.go:135] post-timeout activity - time-elapsed: 58.3813851s, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result: <nil>
W1023 04:18:29.378370       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:30.230363       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
E1023 04:18:30.553672       1 timeout.go:135] post-timeout activity - time-elapsed: 28.7561752s, PUT "/api/v1/nodes/minikube/status" result: <nil>
W1023 04:18:30.986778       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:34.070554       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:34.614977       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:23.924361       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:35.429540       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout". Reconnecting...
W1023 04:18:36.543277       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:36.690519       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:37.127127       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:37.835558       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:39.486223       1 nonstructuralschema_controller.go:204] Shutting down NonStructuralSchemaConditionController
I1023 04:18:39.493036       1 establishing_controller.go:87] Shutting down EstablishingController
I1023 04:18:39.536897       1 crdregistration_controller.go:142] Shutting down crd-autoregister controller
I1023 04:18:39.548328       1 available_controller.go:503] Shutting down AvailableConditionController
I1023 04:18:39.702851       1 autoregister_controller.go:165] Shutting down autoregister controller
I1023 04:18:40.185368       1 dynamic_cafile_content.go:170] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1023 04:18:40.752907       1 dynamic_cafile_content.go:170] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
W1023 04:18:40.774104       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:41.284982       1 dynamic_serving_content.go:144] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1023 04:18:41.285265       1 dynamic_cafile_content.go:170] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
W1023 04:18:41.309492       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:41.582814       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:41.639353       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:42.439433       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:36.179183       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:43.326855       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:43.412229       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:43.808013       1 apiapproval_controller.go:198] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I1023 04:18:39.502795       1 naming_controller.go:302] Shutting down NamingConditionController
W1023 04:18:43.823461       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:43.839521       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I1023 04:18:43.964478       1 apf_controller.go:308] Shutting down API Priority and Fairness config worker
I1023 04:18:39.974837       1 cluster_authentication_trust_controller.go:463] Shutting down cluster_authentication_trust_controller controller
I1023 04:18:40.122547       1 dynamic_cafile_content.go:170] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
W1023 04:18:44.459598       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:44.493434       1 controller.go:122] Shutting down OpenAPI controller
W1023 04:18:38.892808       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:44.584119       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:44.607838       1 crd_finalizer.go:278] Shutting down CRDFinalizer
I1023 04:18:45.205965       1 dynamic_serving_content.go:144] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
W1023 04:18:45.261922       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:45.263040       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
W1023 04:18:45.333512       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:45.699517       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:43.077830       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:43.118421       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:46.582732       1 customresource_discovery_controller.go:245] Shutting down DiscoveryController
W1023 04:18:47.402414       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:47.526274       1 controller.go:181] Shutting down kubernetes service endpoint reconciler
W1023 04:18:47.562182       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
W1023 04:18:47.630381       1 clientconn.go:1326] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: context deadline exceeded". Reconnecting...
I1023 04:18:45.207496       1 controller.go:89] Shutting down OpenAPI AggregationController

* 
* ==> kube-controller-manager [37088de60312] <==
* internal/poll.(*FD).Accept(0xc000743a80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	/usr/local/go/src/internal/poll/fd_unix.go:401 +0x212
net.(*netFD).accept(0xc000743a80, 0x203000, 0x203000, 0x203000)
	/usr/local/go/src/net/fd_unix.go:172 +0x45
net.(*TCPListener).accept(0xc0002dafd8, 0xc000be5cb0, 0x40e19b, 0xc000883310)
	/usr/local/go/src/net/tcpsock_posix.go:139 +0x32
net.(*TCPListener).Accept(0xc0002dafd8, 0x7f369369d518, 0x10, 0x203000, 0x203000)
	/usr/local/go/src/net/tcpsock.go:261 +0x65
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.tcpKeepAliveListener.Accept(0x51d21a8, 0xc0002dafd8, 0xc000be5d10, 0xaa50a877, 0x2d878036a9c65408, 0x0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/secure_serving.go:331 +0x35
crypto/tls.(*listener).Accept(0xc0002da240, 0xc000be5d90, 0x18, 0xc00014c300, 0x880fdb)
	/usr/local/go/src/crypto/tls/tls.go:67 +0x37
net/http.(*Server).Serve(0xc000be68c0, 0x51c1498, 0xc0002da240, 0x0, 0x0)
	/usr/local/go/src/net/http/server.go:2961 +0x285
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.RunServer.func2(0xc0004eed20, 0x51d21a8, 0xc0002dafd8, 0xc000be68c0, 0xc000096360)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/secure_serving.go:306 +0x11d
created by k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.RunServer
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/secure_serving.go:296 +0xff

goroutine 153 [IO wait]:
internal/poll.runtime_pollWait(0x7f3693707d20, 0x72, 0xffffffffffffffff)
	/usr/local/go/src/runtime/netpoll.go:222 +0x55
internal/poll.(*pollDesc).wait(0xc000742a18, 0x72, 0x900, 0x931, 0xffffffffffffffff)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:87 +0x45
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:92
internal/poll.(*FD).Read(0xc000742a00, 0xc000860000, 0x931, 0x931, 0x0, 0x0, 0x0)
	/usr/local/go/src/internal/poll/fd_unix.go:166 +0x1d5
net.(*netFD).Read(0xc000742a00, 0xc000860000, 0x931, 0x931, 0x92c, 0xc000860000, 0x5)
	/usr/local/go/src/net/fd_posix.go:55 +0x4f
net.(*conn).Read(0xc0002ea510, 0xc000860000, 0x931, 0x931, 0x0, 0x0, 0x0)
	/usr/local/go/src/net/net.go:183 +0x91
crypto/tls.(*atLeastReader).Read(0xc0002db7d0, 0xc000860000, 0x931, 0x931, 0x92c, 0xc00006c400, 0x0)
	/usr/local/go/src/crypto/tls/conn.go:776 +0x63
bytes.(*Buffer).ReadFrom(0xc000363078, 0x516f0e0, 0xc0002db7d0, 0x40bf25, 0x423e8c0, 0x4997fc0)
	/usr/local/go/src/bytes/buffer.go:204 +0xbe
crypto/tls.(*Conn).readFromUntil(0xc000362e00, 0x51765c0, 0xc0002ea510, 0x5, 0xc0002ea510, 0x431)
	/usr/local/go/src/crypto/tls/conn.go:798 +0xf3
crypto/tls.(*Conn).readRecordOrCCS(0xc000362e00, 0x0, 0x0, 0x1)
	/usr/local/go/src/crypto/tls/conn.go:605 +0x115
crypto/tls.(*Conn).readRecord(...)
	/usr/local/go/src/crypto/tls/conn.go:573
crypto/tls.(*Conn).Read(0xc000362e00, 0xc0008a3000, 0x1000, 0x1000, 0x0, 0x0, 0x0)
	/usr/local/go/src/crypto/tls/conn.go:1276 +0x165
bufio.(*Reader).Read(0xc000312180, 0xc0001a8c78, 0x9, 0x9, 0x99fbcb, 0xc00085cc78, 0x4071a5)
	/usr/local/go/src/bufio/bufio.go:227 +0x222
io.ReadAtLeast(0x516ef00, 0xc000312180, 0xc0001a8c78, 0x9, 0x9, 0x9, 0xc00092be10, 0x265c7e3607fd00, 0xc00092be10)
	/usr/local/go/src/io/io.go:328 +0x87
io.ReadFull(...)
	/usr/local/go/src/io/io.go:347
k8s.io/kubernetes/vendor/golang.org/x/net/http2.readFrameHeader(0xc0001a8c78, 0x9, 0x9, 0x516ef00, 0xc000312180, 0x0, 0x0, 0x0, 0x0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/frame.go:237 +0x89
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*Framer).ReadFrame(0xc0001a8c40, 0xc000967290, 0x0, 0x0, 0x0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/frame.go:492 +0xa5
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*clientConnReadLoop).run(0xc00085cfa8, 0x0, 0x0)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/transport.go:1821 +0xd8
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*ClientConn).readLoop(0xc000842600)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/transport.go:1743 +0x6f
created by k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*Transport).newClientConn
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/transport.go:695 +0x6c5

* 
* ==> kube-controller-manager [5910b8d69409] <==
* I1023 04:09:16.603018       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-q4mqh" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:09:17.500968       1 event.go:291] "Event occurred" object="kube-system/kube-controller-manager-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:09:17.844155       1 event.go:291] "Event occurred" object="kube-system/etcd-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:09:18.060495       1 event.go:291] "Event occurred" object="kube-system/kube-scheduler-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1023 04:09:18.214193       1 controller_utils.go:148] Failed to update status for pod "kube-apiserver-minikube_kube-system(8c441ec1-80fa-493a-b879-82633b5f4c1a)": Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again
I1023 04:09:18.217220       1 event.go:291] "Event occurred" object="kube-system/kube-apiserver-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:09:18.438463       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-ldmvb" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1023 04:09:18.692948       1 node_lifecycle_controller.go:844] unable to mark all pods NotReady on node minikube: Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again; queuing for retry
I1023 04:09:18.694035       1 node_lifecycle_controller.go:1164] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1023 04:09:18.694628       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-tvzzn" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:09:23.696509       1 node_lifecycle_controller.go:1191] Controller detected that some Nodes are Ready. Exiting master disruption mode.
W1023 04:10:52.934890       1 garbagecollector.go:705] failed to discover preferred resources: Get "https://192.168.49.2:8443/api?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
E1023 04:11:09.934884       1 resource_quota_controller.go:409] failed to discover resources: Get "https://192.168.49.2:8443/api?timeout=32s": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1023 04:11:45.642944       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1023 04:11:48.006214       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-q4mqh" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:11:48.541066       1 event.go:291] "Event occurred" object="kube-system/kube-controller-manager-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:11:48.827112       1 event.go:291] "Event occurred" object="kube-system/etcd-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:11:48.961815       1 event.go:291] "Event occurred" object="kube-system/kube-scheduler-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1023 04:11:49.065819       1 controller_utils.go:148] Failed to update status for pod "kube-apiserver-minikube_kube-system(8c441ec1-80fa-493a-b879-82633b5f4c1a)": Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again
I1023 04:11:49.067591       1 event.go:291] "Event occurred" object="kube-system/kube-apiserver-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:11:49.158666       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-ldmvb" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1023 04:11:49.306449       1 node_lifecycle_controller.go:844] unable to mark all pods NotReady on node minikube: Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again; queuing for retry
I1023 04:11:49.314552       1 node_lifecycle_controller.go:1164] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1023 04:11:49.309715       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-tvzzn" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:11:54.322384       1 node_lifecycle_controller.go:1191] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1023 04:13:02.095157       1 request.go:665] Waited for 1.289567s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/apis/autoscaling/v2beta2?timeout=32s
E1023 04:14:18.652886       1 resource_quota_controller.go:409] unable to retrieve the complete list of server APIs: admissionregistration.k8s.io/v1: Get "https://192.168.49.2:8443/apis/admissionregistration.k8s.io/v1?timeout=32s": context deadline exceeded, apiextensions.k8s.io/v1: Get "https://192.168.49.2:8443/apis/apiextensions.k8s.io/v1?timeout=32s": context deadline exceeded, apps/v1: Get "https://192.168.49.2:8443/apis/apps/v1?timeout=32s": context deadline exceeded, authentication.k8s.io/v1: Get "https://192.168.49.2:8443/apis/authentication.k8s.io/v1?timeout=32s": context deadline exceeded, authorization.k8s.io/v1: Get "https://192.168.49.2:8443/apis/authorization.k8s.io/v1?timeout=32s": context deadline exceeded, autoscaling/v1: Get "https://192.168.49.2:8443/apis/autoscaling/v1?timeout=32s": context deadline exceeded, autoscaling/v2beta1: Get "https://192.168.49.2:8443/apis/autoscaling/v2beta1?timeout=32s": context deadline exceeded, autoscaling/v2beta2: Get "https://192.168.49.2:8443/apis/autoscaling/v2beta2?timeout=32s": context deadline exceeded, batch/v1beta1: Get "https://192.168.49.2:8443/apis/batch/v1beta1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), certificates.k8s.io/v1: Get "https://192.168.49.2:8443/apis/certificates.k8s.io/v1?timeout=32s": context deadline exceeded, coordination.k8s.io/v1: Get "https://192.168.49.2:8443/apis/coordination.k8s.io/v1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), discovery.k8s.io/v1: Get "https://192.168.49.2:8443/apis/discovery.k8s.io/v1?timeout=32s": context deadline exceeded, discovery.k8s.io/v1beta1: Get "https://192.168.49.2:8443/apis/discovery.k8s.io/v1beta1?timeout=32s": context deadline exceeded, events.k8s.io/v1: Get "https://192.168.49.2:8443/apis/events.k8s.io/v1?timeout=32s": context deadline exceeded, events.k8s.io/v1beta1: Get "https://192.168.49.2:8443/apis/events.k8s.io/v1beta1?timeout=32s": context deadline exceeded, flowcontrol.apiserver.k8s.io/v1beta1: Get "https://192.168.49.2:8443/apis/flowcontrol.apiserver.k8s.io/v1beta1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), networking.k8s.io/v1: Get "https://192.168.49.2:8443/apis/networking.k8s.io/v1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), node.k8s.io/v1: Get "https://192.168.49.2:8443/apis/node.k8s.io/v1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), node.k8s.io/v1beta1: Get "https://192.168.49.2:8443/apis/node.k8s.io/v1beta1?timeout=32s": context deadline exceeded, policy/v1: Get "https://192.168.49.2:8443/apis/policy/v1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), policy/v1beta1: Get "https://192.168.49.2:8443/apis/policy/v1beta1?timeout=32s": net/http: request canceled (Client.Timeout exceeded while awaiting headers), scheduling.k8s.io/v1: Get "https://192.168.49.2:8443/apis/scheduling.k8s.io/v1?timeout=32s": context deadline exceeded, storage.k8s.io/v1: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1?timeout=32s": context deadline exceeded, storage.k8s.io/v1beta1: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1?timeout=32s": context deadline exceeded, v1: Get "https://192.168.49.2:8443/api/v1?timeout=32s": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1023 04:14:49.919781       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1023 04:14:54.838352       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-q4mqh" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:14:56.321356       1 event.go:291] "Event occurred" object="kube-system/kube-controller-manager-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:14:56.668144       1 event.go:291] "Event occurred" object="kube-system/etcd-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:14:57.049866       1 event.go:291] "Event occurred" object="kube-system/kube-scheduler-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
W1023 04:14:57.843217       1 controller_utils.go:148] Failed to update status for pod "kube-apiserver-minikube_kube-system(8c441ec1-80fa-493a-b879-82633b5f4c1a)": Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again
I1023 04:14:57.873414       1 event.go:291] "Event occurred" object="kube-system/kube-apiserver-minikube" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:14:59.095413       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-ldmvb" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1023 04:14:59.335182       1 node_lifecycle_controller.go:844] unable to mark all pods NotReady on node minikube: Operation cannot be fulfilled on pods "kube-apiserver-minikube": the object has been modified; please apply your changes to the latest version and try again; queuing for retry
I1023 04:14:59.340951       1 event.go:291] "Event occurred" object="kube-system/coredns-78fcd69978-tvzzn" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1023 04:14:59.342028       1 node_lifecycle_controller.go:1164] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1023 04:15:04.344479       1 node_lifecycle_controller.go:1191] Controller detected that some Nodes are Ready. Exiting master disruption mode.
E1023 04:17:05.608529       1 resource_quota_controller.go:409] failed to discover resources: Get "https://192.168.49.2:8443/api?timeout=32s": context deadline exceeded
E1023 04:17:30.980187       1 node_lifecycle_controller.go:1107] Error updating node minikube: Put "https://192.168.49.2:8443/api/v1/nodes/minikube/status": http2: client connection lost
W1023 04:17:30.884906       1 garbagecollector.go:705] failed to discover preferred resources: Get "https://192.168.49.2:8443/api?timeout=32s": http2: client connection lost
E1023 04:18:08.519713       1 resource_quota_controller.go:409] failed to discover resources: Get "https://192.168.49.2:8443/api?timeout=32s": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
E1023 04:18:20.687342       1 node_lifecycle_controller.go:801] Failed while getting a Node to retry updating node health. Probably Node minikube was deleted.
W1023 04:18:20.656942       1 garbagecollector.go:705] failed to discover preferred resources: Get "https://192.168.49.2:8443/api?timeout=32s": net/http: TLS handshake timeout
E1023 04:18:20.797443       1 node_lifecycle_controller.go:806] Update health of Node '' from Controller error: Get "https://192.168.49.2:8443/api/v1/nodes/minikube": net/http: TLS handshake timeout. Skipping - no pods will be evicted.
I1023 04:18:27.332871       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
E1023 04:18:42.931067       1 node_lifecycle_controller.go:1107] Error updating node minikube: Put "https://192.168.49.2:8443/api/v1/nodes/minikube/status": net/http: TLS handshake timeout
E1023 04:18:47.497392       1 node_lifecycle_controller.go:801] Failed while getting a Node to retry updating node health. Probably Node minikube was deleted.
E1023 04:18:47.509850       1 node_lifecycle_controller.go:806] Update health of Node '' from Controller error: Get "https://192.168.49.2:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused. Skipping - no pods will be evicted.
E1023 04:18:47.538072       1 resource_quota_controller.go:409] failed to discover resources: Get "https://192.168.49.2:8443/api?timeout=32s": dial tcp 192.168.49.2:8443: connect: connection refused
W1023 04:18:51.005447       1 garbagecollector.go:705] failed to discover preferred resources: Get "https://192.168.49.2:8443/api?timeout=32s": dial tcp 192.168.49.2:8443: connect: connection refused
I1023 04:18:52.582478       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
E1023 04:18:52.766284       1 node_lifecycle_controller.go:1107] Error updating node minikube: Put "https://192.168.49.2:8443/api/v1/nodes/minikube/status": dial tcp 192.168.49.2:8443: connect: connection refused
E1023 04:18:52.793919       1 node_lifecycle_controller.go:801] Failed while getting a Node to retry updating node health. Probably Node minikube was deleted.
E1023 04:18:52.793984       1 node_lifecycle_controller.go:806] Update health of Node '' from Controller error: Get "https://192.168.49.2:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused. Skipping - no pods will be evicted.
I1023 04:18:57.843701       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
E1023 04:18:57.879683       1 node_lifecycle_controller.go:1107] Error updating node minikube: Put "https://192.168.49.2:8443/api/v1/nodes/minikube/status": dial tcp 192.168.49.2:8443: connect: connection refused
E1023 04:18:57.941962       1 node_lifecycle_controller.go:801] Failed while getting a Node to retry updating node health. Probably Node minikube was deleted.
E1023 04:18:57.946328       1 node_lifecycle_controller.go:806] Update health of Node '' from Controller error: Get "https://192.168.49.2:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused. Skipping - no pods will be evicted.

* 
* ==> kube-proxy [e6048d9bfd74] <==
* I1023 04:06:50.579711       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I1023 04:06:50.579862       1 server_others.go:212] Using iptables Proxier.
I1023 04:06:50.579899       1 server_others.go:219] creating dualStackProxier for iptables.
W1023 04:06:50.579936       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I1023 04:06:50.652506       1 server.go:649] Version: v1.22.2
I1023 04:06:50.927015       1 config.go:315] Starting service config controller
I1023 04:06:50.927728       1 shared_informer.go:240] Waiting for caches to sync for service config
I1023 04:06:51.021989       1 config.go:224] Starting endpoint slice config controller
I1023 04:06:51.022070       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1023 04:06:53.225166       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I1023 04:06:53.231035       1 shared_informer.go:247] Caches are synced for service config 
E1023 04:06:53.529103       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube.16b08d496754cce8", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, EventTime:v1.MicroTime{Time:time.Time{wall:0xc0550076c148b218, ext:13652218001, loc:(*time.Location)(0x2d81340)}}, Series:(*v1.EventSeries)(nil), ReportingController:"kube-proxy", ReportingInstance:"kube-proxy-minikube", Action:"StartKubeProxy", Reason:"Starting", Regarding:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube", UID:"minikube", APIVersion:"", ResourceVersion:"", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"", Type:"Normal", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeprecatedLastTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeprecatedCount:0}': 'Event "minikube.16b08d496754cce8" is invalid: involvedObject.namespace: Invalid value: "": does not match event.namespace' (will not retry!)
I1023 04:07:23.260357       1 trace.go:205] Trace[756892854]: "iptables ChainExists" (23-Oct-2021 04:07:20.777) (total time: 2481ms):
Trace[756892854]: [2.4818533s] [2.4818533s] END
I1023 04:07:23.339725       1 trace.go:205] Trace[1207171058]: "iptables ChainExists" (23-Oct-2021 04:07:20.771) (total time: 2242ms):
Trace[1207171058]: [2.2429937s] [2.2429937s] END
I1023 04:08:17.337796       1 trace.go:205] Trace[832958146]: "iptables ChainExists" (23-Oct-2021 04:07:50.605) (total time: 22757ms):
Trace[832958146]: [22.7577631s] [22.7577631s] END
I1023 04:08:17.338302       1 trace.go:205] Trace[626110277]: "iptables ChainExists" (23-Oct-2021 04:07:50.606) (total time: 22841ms):
Trace[626110277]: [22.8419678s] [22.8419678s] END
I1023 04:08:55.906982       1 trace.go:205] Trace[1214533323]: "iptables ChainExists" (23-Oct-2021 04:08:51.775) (total time: 3773ms):
Trace[1214533323]: [3.7738425s] [3.7738425s] END
I1023 04:08:55.907159       1 trace.go:205] Trace[1533486867]: "iptables ChainExists" (23-Oct-2021 04:08:51.780) (total time: 3768ms):
Trace[1533486867]: [3.7686835s] [3.7686835s] END
I1023 04:10:53.499094       1 trace.go:205] Trace[17173048]: "iptables ChainExists" (23-Oct-2021 04:10:50.414) (total time: 2573ms):
Trace[17173048]: [2.5735463s] [2.5735463s] END
I1023 04:10:53.499408       1 trace.go:205] Trace[1187299987]: "iptables ChainExists" (23-Oct-2021 04:10:50.440) (total time: 2383ms):
Trace[1187299987]: [2.3830915s] [2.3830915s] END
I1023 04:11:24.135871       1 trace.go:205] Trace[1547905109]: "iptables ChainExists" (23-Oct-2021 04:11:21.236) (total time: 2560ms):
Trace[1547905109]: [2.5606857s] [2.5606857s] END
I1023 04:11:24.138383       1 trace.go:205] Trace[343441790]: "iptables ChainExists" (23-Oct-2021 04:11:21.339) (total time: 2506ms):
Trace[343441790]: [2.5062269s] [2.5062269s] END
I1023 04:12:24.658220       1 trace.go:205] Trace[518288115]: "iptables ChainExists" (23-Oct-2021 04:12:20.346) (total time: 4226ms):
Trace[518288115]: [4.2269521s] [4.2269521s] END
I1023 04:12:24.664420       1 trace.go:205] Trace[223908896]: "iptables ChainExists" (23-Oct-2021 04:12:21.921) (total time: 2648ms):
Trace[223908896]: [2.648658s] [2.648658s] END
I1023 04:12:54.554786       1 trace.go:205] Trace[2141973367]: "iptables ChainExists" (23-Oct-2021 04:12:50.835) (total time: 3309ms):
Trace[2141973367]: [3.3092896s] [3.3092896s] END
I1023 04:12:54.571220       1 trace.go:205] Trace[24346319]: "iptables ChainExists" (23-Oct-2021 04:12:50.608) (total time: 3636ms):
Trace[24346319]: [3.6364665s] [3.6364665s] END
I1023 04:13:34.376887       1 trace.go:205] Trace[429342523]: "iptables ChainExists" (23-Oct-2021 04:13:25.291) (total time: 9024ms):
Trace[429342523]: [9.0249968s] [9.0249968s] END
I1023 04:13:34.801958       1 trace.go:205] Trace[2065213307]: "iptables ChainExists" (23-Oct-2021 04:13:25.025) (total time: 8850ms):
Trace[2065213307]: [8.8503134s] [8.8503134s] END
I1023 04:16:57.896465       1 trace.go:205] Trace[1579764459]: "iptables ChainExists" (23-Oct-2021 04:16:50.904) (total time: 6561ms):
Trace[1579764459]: [6.5612311s] [6.5612311s] END
I1023 04:16:57.896812       1 trace.go:205] Trace[762227494]: "iptables ChainExists" (23-Oct-2021 04:16:50.876) (total time: 6314ms):
Trace[762227494]: [6.3143291s] [6.3143291s] END
I1023 04:17:24.911996       1 trace.go:205] Trace[1882370179]: "iptables ChainExists" (23-Oct-2021 04:17:19.956) (total time: 4279ms):
Trace[1882370179]: [4.2796599s] [4.2796599s] END
I1023 04:17:25.023148       1 trace.go:205] Trace[615406265]: "iptables ChainExists" (23-Oct-2021 04:17:19.956) (total time: 4324ms):
Trace[615406265]: [4.3241611s] [4.3241611s] END
I1023 04:17:58.958904       1 trace.go:205] Trace[2031411400]: "iptables ChainExists" (23-Oct-2021 04:17:50.901) (total time: 7563ms):
Trace[2031411400]: [7.563717s] [7.563717s] END
I1023 04:17:58.959221       1 trace.go:205] Trace[1055731964]: "iptables ChainExists" (23-Oct-2021 04:17:50.656) (total time: 7552ms):
Trace[1055731964]: [7.5525523s] [7.5525523s] END
I1023 04:18:23.229325       1 trace.go:205] Trace[500018389]: "iptables ChainExists" (23-Oct-2021 04:18:20.063) (total time: 3147ms):
Trace[500018389]: [3.1470869s] [3.1470869s] END
I1023 04:18:23.551903       1 trace.go:205] Trace[2063132607]: "iptables ChainExists" (23-Oct-2021 04:18:20.057) (total time: 3494ms):
Trace[2063132607]: [3.4942196s] [3.4942196s] END

* 
* ==> kube-scheduler [49aa8c6c969f] <==
* E1023 04:02:26.666564       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1023 04:02:26.666610       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1023 04:02:26.699193       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1023 04:02:26.819464       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 04:02:28.662646       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1023 04:02:29.209259       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:29.471439       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1023 04:02:30.080085       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1023 04:02:30.233816       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1023 04:02:30.335411       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:30.733894       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1023 04:02:30.768283       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1023 04:02:31.186149       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1023 04:02:32.002729       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:32.002808       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1023 04:02:32.106971       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1023 04:02:32.463369       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:32.694474       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1023 04:02:33.492187       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 04:02:36.005366       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1023 04:02:38.611031       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1023 04:02:38.918841       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1023 04:02:39.443519       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1023 04:02:40.195467       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:40.235488       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1023 04:02:40.283543       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:41.029889       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 04:02:41.547028       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1023 04:02:42.268683       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:42.392078       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1023 04:02:42.620327       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1023 04:02:42.628352       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1023 04:02:42.752543       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1023 04:02:42.950728       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1023 04:02:57.840478       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 04:02:57.917780       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1023 04:03:00.834584       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1023 04:03:00.840660       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1023 04:03:00.854613       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1023 04:03:01.875395       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1023 04:03:01.875423       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1023 04:03:03.718548       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1023 04:03:03.718909       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1023 04:03:03.721119       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1023 04:03:04.708333       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1023 04:03:04.895691       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1023 04:03:05.552722       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1023 04:03:06.332272       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1023 04:03:08.751507       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1023 04:03:30.261456       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1023 04:03:30.298280       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1023 04:03:31.636302       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1023 04:03:35.505785       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1023 04:03:35.500448       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1023 04:03:35.651139       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1023 04:03:40.815248       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
I1023 04:03:47.008772       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I1023 04:05:23.783831       1 trace.go:205] Trace[936622711]: "Scheduling" namespace:kube-system,name:kube-proxy-ldmvb (23-Oct-2021 04:05:23.598) (total time: 140ms):
Trace[936622711]: ---"Computing predicates done" 137ms (04:05:23.739)
Trace[936622711]: [140.6891ms] [140.6891ms] END

* 
* ==> kubelet <==
* -- Logs begin at Sat 2021-10-23 03:51:10 UTC, end at Sat 2021-10-23 04:19:00 UTC. --
Oct 23 04:14:28 minikube kubelet[4743]: E1023 04:14:28.329813    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-tvzzn.16b08d5d8aa11db4", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"688", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-tvzzn", UID:"08bce1e1-3663-4a14-b251-3218b8a3a5a8", APIVersion:"v1", ResourceVersion:"495", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.2:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558897, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc05500dc36e703fc, ext:584440289101, loc:(*time.Location)(0x55d72806f680)}}, Count:6, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-tvzzn.16b08d5d8aa11db4": read tcp 192.168.49.2:47500->192.168.49.2:8443: use of closed network connection'(may retry after sleeping)
Oct 23 04:14:28 minikube kubelet[4743]: I1023 04:14:28.395558    4743 controller.go:114] failed to update lease using latest lease, fallback to ensure lease, err: failed 5 attempts to update lease
Oct 23 04:14:38 minikube kubelet[4743]: E1023 04:14:38.641377    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": context deadline exceeded"
Oct 23 04:14:39 minikube kubelet[4743]: E1023 04:14:39.491768    4743 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded
Oct 23 04:14:48 minikube kubelet[4743]: E1023 04:14:48.550511    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:14:48 minikube kubelet[4743]: E1023 04:14:48.550694    4743 kubelet_node_status.go:457] "Unable to update node status" err="update node status exceeds retry count"
Oct 23 04:14:48 minikube kubelet[4743]: E1023 04:14:48.562190    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-tvzzn.16b08d5d8aa11db4", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"688", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-tvzzn", UID:"08bce1e1-3663-4a14-b251-3218b8a3a5a8", APIVersion:"v1", ResourceVersion:"495", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.2:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558897, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc05500dc36e703fc, ext:584440289101, loc:(*time.Location)(0x55d72806f680)}}, Count:6, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-tvzzn.16b08d5d8aa11db4": read tcp 192.168.49.2:47978->192.168.49.2:8443: use of closed network connection'(may retry after sleeping)
Oct 23 04:14:49 minikube kubelet[4743]: E1023 04:14:49.898964    4743 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded
Oct 23 04:16:46 minikube kubelet[4743]: E1023 04:16:45.945982    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:16:47 minikube kubelet[4743]: E1023 04:16:46.111489    4743 controller.go:187] failed to update lease, error: Put "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
Oct 23 04:16:57 minikube kubelet[4743]: E1023 04:16:57.567848    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": context deadline exceeded"
Oct 23 04:16:58 minikube kubelet[4743]: E1023 04:16:58.676801    4743 controller.go:187] failed to update lease, error: Put "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": read tcp 192.168.49.2:48160->192.168.49.2:8443: use of closed network connection (Client.Timeout exceeded while awaiting headers)
Oct 23 04:17:05 minikube kubelet[4743]: I1023 04:17:05.215846    4743 trace.go:205] Trace[771570649]: "iptables ChainExists" (23-Oct-2021 04:17:00.923) (total time: 4122ms):
Oct 23 04:17:05 minikube kubelet[4743]: Trace[771570649]: [4.1222336s] [4.1222336s] END
Oct 23 04:17:05 minikube kubelet[4743]: I1023 04:17:05.216609    4743 trace.go:205] Trace[1033508574]: "iptables ChainExists" (23-Oct-2021 04:17:01.257) (total time: 3775ms):
Oct 23 04:17:05 minikube kubelet[4743]: Trace[1033508574]: [3.7754086s] [3.7754086s] END
Oct 23 04:17:07 minikube kubelet[4743]: E1023 04:17:07.593009    4743 controller.go:187] failed to update lease, error: Put "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": read tcp 192.168.49.2:50852->192.168.49.2:8443: use of closed network connection
Oct 23 04:17:07 minikube kubelet[4743]: E1023 04:17:07.600190    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:17:16 minikube kubelet[4743]: E1023 04:17:16.895649    4743 controller.go:187] failed to update lease, error: Put "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Oct 23 04:17:17 minikube kubelet[4743]: E1023 04:17:17.093913    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:17:27 minikube kubelet[4743]: E1023 04:17:27.868038    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:17:27 minikube kubelet[4743]: E1023 04:17:27.868126    4743 kubelet_node_status.go:457] "Unable to update node status" err="update node status exceeds retry count"
Oct 23 04:17:28 minikube kubelet[4743]: E1023 04:17:27.224854    4743 controller.go:187] failed to update lease, error: Put "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: TLS handshake timeout
Oct 23 04:17:31 minikube kubelet[4743]: I1023 04:17:30.503428    4743 controller.go:114] failed to update lease using latest lease, fallback to ensure lease, err: failed 5 attempts to update lease
Oct 23 04:17:41 minikube kubelet[4743]: E1023 04:17:41.807143    4743 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Oct 23 04:17:48 minikube kubelet[4743]: E1023 04:17:48.176652    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:17:52 minikube kubelet[4743]: E1023 04:17:52.283931    4743 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Oct 23 04:17:58 minikube kubelet[4743]: E1023 04:17:58.516179    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:17:59 minikube kubelet[4743]: E1023 04:17:58.869760    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-q4mqh", UID:"2b46c0e4-c735-4b6c-be26-ecaa702fbe26", APIVersion:"v1", ResourceVersion:"498", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558891, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0550116a95c8874, ext:818674073801, loc:(*time.Location)(0x55d72806f680)}}, Count:8, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c": read tcp 192.168.49.2:51506->192.168.49.2:8443: use of closed network connection'(may retry after sleeping)
Oct 23 04:18:03 minikube kubelet[4743]: E1023 04:18:03.052068    4743 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded
Oct 23 04:18:08 minikube kubelet[4743]: E1023 04:18:08.591252    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:18:08 minikube kubelet[4743]: E1023 04:18:08.591141    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-q4mqh", UID:"2b46c0e4-c735-4b6c-be26-ecaa702fbe26", APIVersion:"v1", ResourceVersion:"498", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558891, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0550116a95c8874, ext:818674073801, loc:(*time.Location)(0x55d72806f680)}}, Count:8, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c": read tcp 192.168.49.2:52214->192.168.49.2:8443: use of closed network connection'(may retry after sleeping)
Oct 23 04:18:14 minikube kubelet[4743]: E1023 04:18:14.007302    4743 controller.go:144] failed to ensure lease exists, will retry in 1.6s, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded
Oct 23 04:18:18 minikube kubelet[4743]: E1023 04:18:18.832587    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:18:25 minikube kubelet[4743]: E1023 04:18:25.533992    4743 controller.go:144] failed to ensure lease exists, will retry in 3.2s, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": context deadline exceeded
Oct 23 04:18:29 minikube kubelet[4743]: E1023 04:18:29.089186    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-q4mqh", UID:"2b46c0e4-c735-4b6c-be26-ecaa702fbe26", APIVersion:"v1", ResourceVersion:"498", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558891, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0550116a95c8874, ext:818674073801, loc:(*time.Location)(0x55d72806f680)}}, Count:8, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c": net/http: TLS handshake timeout'(may retry after sleeping)
Oct 23 04:18:29 minikube kubelet[4743]: E1023 04:18:29.695650    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Oct 23 04:18:29 minikube kubelet[4743]: E1023 04:18:29.695904    4743 kubelet_node_status.go:457] "Unable to update node status" err="update node status exceeds retry count"
Oct 23 04:18:39 minikube kubelet[4743]: E1023 04:18:39.365090    4743 controller.go:144] failed to ensure lease exists, will retry in 6.4s, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Oct 23 04:18:40 minikube kubelet[4743]: I1023 04:18:40.571361    4743 status_manager.go:601] "Failed to get status for pod" podUID=fe49842e48a1ca6bac988e8ba88ecc53 pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": net/http: TLS handshake timeout"
Oct 23 04:18:46 minikube kubelet[4743]: E1023 04:18:46.584257    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-q4mqh", UID:"2b46c0e4-c735-4b6c-be26-ecaa702fbe26", APIVersion:"v1", ResourceVersion:"498", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558891, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0550116a95c8874, ext:818674073801, loc:(*time.Location)(0x55d72806f680)}}, Count:8, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c": read tcp 192.168.49.2:52902->192.168.49.2:8443: read: connection reset by peer'(may retry after sleeping)
Oct 23 04:18:47 minikube kubelet[4743]: I1023 04:18:47.405684    4743 status_manager.go:601] "Failed to get status for pod" podUID=fe49842e48a1ca6bac988e8ba88ecc53 pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:47 minikube kubelet[4743]: E1023 04:18:47.405920    4743 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 23 04:18:47 minikube kubelet[4743]: E1023 04:18:47.577337    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:47 minikube kubelet[4743]: E1023 04:18:47.755686    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:47 minikube kubelet[4743]: E1023 04:18:47.988210    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:48 minikube kubelet[4743]: E1023 04:18:47.989775    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:48 minikube kubelet[4743]: E1023 04:18:47.993025    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:48 minikube kubelet[4743]: E1023 04:18:47.993077    4743 kubelet_node_status.go:457] "Unable to update node status" err="update node status exceeds retry count"
Oct 23 04:18:50 minikube kubelet[4743]: I1023 04:18:50.068735    4743 status_manager.go:601] "Failed to get status for pod" podUID=fe49842e48a1ca6bac988e8ba88ecc53 pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:54 minikube kubelet[4743]: E1023 04:18:54.665018    4743 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": dial tcp 192.168.49.2:8443: connect: connection refused
Oct 23 04:18:56 minikube kubelet[4743]: E1023 04:18:56.644030    4743 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"coredns-78fcd69978-q4mqh", UID:"2b46c0e4-c735-4b6c-be26-ecaa702fbe26", APIVersion:"v1", ResourceVersion:"498", FieldPath:"spec.containers{coredns}"}, Reason:"Unhealthy", Message:"Readiness probe failed: Get \"http://172.17.0.3:8181/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770558891, loc:(*time.Location)(0x55d72806f680)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0550116a95c8874, ext:818674073801, loc:(*time.Location)(0x55d72806f680)}}, Count:8, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-78fcd69978-q4mqh.16b08d5c1c8bdd4c": dial tcp 192.168.49.2:8443: connect: connection refused'(may retry after sleeping)
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.001404    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.011245    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.012100    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.032964    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.038274    4743 kubelet_node_status.go:470] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:18:58 minikube kubelet[4743]: E1023 04:18:58.038312    4743 kubelet_node_status.go:457] "Unable to update node status" err="update node status exceeds retry count"
Oct 23 04:19:00 minikube kubelet[4743]: I1023 04:19:00.023433    4743 status_manager.go:601] "Failed to get status for pod" podUID=fe49842e48a1ca6bac988e8ba88ecc53 pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 23 04:19:00 minikube kubelet[4743]: I1023 04:19:00.684942    4743 status_manager.go:601] "Failed to get status for pod" podUID=fe49842e48a1ca6bac988e8ba88ecc53 pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"

